{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 카일 스쿨 6회차\n",
    "- [![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fzzsza.github.io%2Fkyle-school%2Fweek6)](https://hits.seeyoufarm.com)\n",
    "- #0. 영양제\n",
    "- #1. Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 0. 영양제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 영양제/비타민을 왜 섭취해야 하는가\n",
    "    - 최고의 건강 관리 : 균형잡힌 식사와 꾸준한 운동\n",
    "    - 하지만... 꾸준한 운동..\n",
    "    - 과로, 스트레스에 시달리는 경우 영양소 소모가 많음 => 부족한 부분 발생\n",
    "    - 자취생 => 과일 못먹음 + 배달 => 균형 잡히지 못한 식사\n",
    "    - 비타민 D => 햇빛을 봐야하는데 => 우린 일을 하네"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 어떤 비타민을 먹어야 하는가\n",
    "- 비타민B\n",
    "    - 육체 피로 회복\n",
    "    - 학업, 취업, 업무 등으로 육체피로가 만성화 => 마그네슘과 같이 섞기\n",
    "    - 저는 비맥스 메타를 먹고 있는데 효과 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 마그네슘\n",
    "    - 세포 에너지 생성에 관여\n",
    "    - 마그네슘 부족시 피로, 근육통, 경련, 수면장애, 우울감 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 루테인\n",
    "    - 시세포가 밀집된 황반의 기능을 유지\n",
    "- 비타민C\n",
    "    - 항산화 효능, 면역령 강화\n",
    "- 오메가3\n",
    "    - 안구건조증에 효과적, 당뇨에 도움, 치매 방지 등\n",
    "- 다 먹으라는 것은 아니고 하나씩 찾아보고, 자신에게 맞는 것을 섭취!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 비타민을 어떻게 구입할 것인가\n",
    "    - 아이허브 VS 로켓 쿠팡직구\n",
    "        - 아이허브가 무조건 저렴한 것은 아니고, 로켓 쿠팡직구가 저렴한 경우도 있음\n",
    "        - 아이허브는 40달러 이상 구매하면 무료 배송\n",
    "- 사실 제일 중요한 것은\n",
    "    - 꾸준한 운동\n",
    "    - 좋은 식습관\n",
    "    - 영양제\n",
    "    - 3위일체..!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 추천 자료\n",
    "    - 약사가 들려주는 약이야기(고약사님) [유튜브](https://www.youtube.com/watch?v=TqtSLSsjtZs)\n",
    "    - 약사가 들려주는 약이야기(고약사님) [인스타](https://www.instagram.com/yakstory119/)\n",
    "    - 쿠마님 [블로그](https://blog.naver.com/hs_kuma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Airflow\n",
    "- 오늘 할 이야기\n",
    "    - Airflow란?\n",
    "    - Airflow Architecture\n",
    "    - DAG\n",
    "    - Airflow BashOperator, PythonOperator 사용하기\n",
    "    - Jinja Template 사용하기\n",
    "    - Airflow로 토이 ETL 파이프라인 만들기\n",
    "- 사실 더 하고싶지만.. 1시간은 생각보다 짧기 때문에 이정도만 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Apache Airflow란?\n",
    "    - 에어비앤비에서 만든 Workflow Management Tool\n",
    "        - Workflow : 일련의 Task들의 연결\n",
    "    - 활용할 수 있는 포인트\n",
    "        - 데이터 엔지니어링 : ETL 파이프라인\n",
    "            - 데이터를 source에서 가져와서 데이터 마트, 데이터 웨어하우스 등에 저장\n",
    "        - 머신러닝 엔지니어링\n",
    "            - 머신러닝 모델 주기적인 학습(1주 간격), 예측(30분 간격)\n",
    "            - 실시간 API가 아닌 Batch성 예측\n",
    "        - 간단한 cron 작업\n",
    "            - crontab에 특정 작업 반복 등을 실행\n",
    "        - 여러 작업들의 연결성(의존성) 관리\n",
    "            - 앞의 작업이 성공해야 뒤 작업을 하도록 설정\n",
    "        - 여러가지 작업을 효율적으로 관리(시각화 등)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Apache Airflow의 장점\n",
    "    - Python 기반\n",
    "    - Scheduling : 특정 간격으로 계속 실행\n",
    "    - Backfill : 과거 작업 실행\n",
    "    - 특정 Task 실패시 => Task만 재실행 / DAG 재실행 등 실패 로직도 있음\n",
    "    - 데이터 엔지니어링에서 많이 사용됨\n",
    "    - Google Cloud Platform에 있는 대부분의 기능을 지원\n",
    "    - Google Cloud Platform엔 Managed Service(관리형 서비스)인 Composer 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow UI 설명\n",
    "    - <img src=\"https://www.dropbox.com/s/z9j9ncgn7ql61fg/Screenshot%202020-02-09%2016.56.48.png?raw=1\" width=\"1000\" height=\"1000\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 각종 Task 연결(Graph View)\n",
    "    - <img src=\"https://www.dropbox.com/s/olwm5dsgq4rzlhh/Screenshot%202020-02-09%2017.03.16.png?raw=1\">\n",
    "    - 빨간색 : failed => 앞에 작업들이 실패해서 뒤 작업 run_after_loop이 노란색 upstream_failed 되고 실행되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- UTC\n",
    "    - 협정 세계시로 1972년 1월 1일부터 시행된 국제 표준시\n",
    "    - 서버에서 시간 처리할 땐, 거의 UTC를 사용함\n",
    "    - 한국 시간은 **UTC+9hour** \n",
    "    - Airflow에서 UTC를 사용하기 때문에, CRON 표시할 때 UTC 기준으로 작성\n",
    "        - 예 : UTC `30 1 * * *` => 한국은 `30 10 * * *` => 한국 오전 10시 30분\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow 실행\n",
    "    - airflow webserver와 airflow scheduler 2개 실행해야 함\n",
    "    - 터미널 1개에 webserver를 띄우고, command+t로 새로운 터미널을 띄워서 scheduler를 띄우기\n",
    "\n",
    "    ```\n",
    "    airflow webserver\n",
    "    airflow scheduler\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow 실행해보기\n",
    "    - tutorial DAG을 실행(Links 아래에 있는 재생 버튼 클릭)\n",
    "    - 혹시 ValueError: unknown locale: UTF-8 에러가 날경우 `~/.zshrc` 또는 `~/.bash_profile`에 아래 설정 추가\n",
    "\n",
    "        ```\n",
    "        export LC_ALL=en_US.UTF-8\n",
    "        export LANG=en_US.UTF-8\n",
    "        ```\n",
    "\n",
    "    - 그 후 터미널에서 아래 커맨드 실행하고 webserver 다시 실행\n",
    "\n",
    "        ```\n",
    "        source ~/.zshrc\n",
    "        # 또는 source ~/.bash_profile\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow Architecture\n",
    "    - <img src=\"https://www.dropbox.com/s/ofbftr7xz9az4jc/Screenshot%202020-02-12%2022.12.42.png?raw=1\" width=\"600\" height=\"600\">\n",
    "    - Airflow Webserver\n",
    "        - 웹 UI를 표현하고, workflow 상태 표시하고 실행, 재시작, 수동 조작, 로그 확인 등 가능\n",
    "    - Airflow Scheduler\n",
    "        - 작업 기준이 충족되는지 여부를 확인\n",
    "        - 종속 작업이 성공적으로 완료되었고, 예약 간격이 주어지면 실행할 수 있는 작업인지, 실행 조건이 충족되는지 등\n",
    "        - 위 충족 여부가 DB에 기록되면, task들이 worker에게 선택되서 작업을 실행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- DAG\n",
    "    - <img src=\"https://www.dropbox.com/s/nmuo01asr9psqew/Screenshot%202020-02-12%2021.42.31.png?raw=1\" width=\"400\" height=\"400\">\n",
    "    - Airflow의 DAG으로 모델링됨\n",
    "    - Directed Acyclic Graphs\n",
    "    - 방향이 있는 비순환 그래프\n",
    "    - 비순환이기 때문에 마지막 Task가 다시 처음 Task로 이어지지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 코드로 보는 DAG\n",
    "    - <img src=\"https://www.dropbox.com/s/zgfc3grlwd1v88r/Screenshot%202020-02-12%2022.39.39.png?raw=1\" width=\"600\" height=\"600\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 1) Default Argument 정의\n",
    "    - start_date가 중요! 과거 날짜를 설정하면 그 날부터 실행\n",
    "    - retries, retry_delay : 실패할 경우 몇분 뒤에 재실행할지?\n",
    "    - priority_weight : 우선 순위\n",
    "    - 외에도 다양한 옵션이 있는데, [문서](https://airflow.apache.org/docs/stable/tutorial.html) 참고\n",
    "    \n",
    "    ```\n",
    "    default_args = {\n",
    "        'owner': 'your_name',\n",
    "        'depends_on_past': False,\n",
    "        'start_date': datetime(2018, 12, 1),\n",
    "        'email': ['your@mail.com'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        'priority_weight': 10,\n",
    "        'end_date': datetime(2018, 12, 3),\n",
    "        # end_date가 없으면 계속 진행함\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2) DAG 객체 생성\n",
    "    - 첫 인자는 dag_id인데 고유한 id 작성\n",
    "    - default_args는 위에서 정의한 argument를 넣고\n",
    "    - schedule_interval은 crontab 표현 사용\n",
    "        - schedule_interval='@once'는 한번만 실행. 디버깅용으로 자주 사용\n",
    "        - `5 4 * * *` 같은 표현을 사용\n",
    "        - 더 궁금하면 [crontab guru](https://crontab.guru/) 참고\n",
    "    \n",
    "    ```\n",
    "    dag = DAG('bash_dag', default_args=default_args, schedule_interval='@once'))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 3) Operator로 Task 정의\n",
    "    - Operator가 Instance가 되면 Task라 부름\n",
    "    - BashOperator : Bash Command 실행\n",
    "    - PythonOperator : Python 함수 실행\n",
    "    - BigQueryOperator : BigQuery 쿼리 날린 후 Table 저장\n",
    "    - 외에도 다양한 operator가 있고, operator마다 옵션이 다름\n",
    "    - [Airflow Document](https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html), [Integration Operator](https://airflow.apache.org/docs/stable/integration.html) 참고\n",
    "    - mysql_to_hive 등도 있음\n",
    "    \n",
    "    ```\n",
    "    task1 = BashOperator(\n",
    "        task_id='print_date',\n",
    "        bash_command='date',\n",
    "        dag=dag)\n",
    "\n",
    "    task2 = BashOperator(\n",
    "        task_id='sleep',\n",
    "        bash_command='sleep 5',\n",
    "        retries=2,\n",
    "        dag=dag)\n",
    "\n",
    "    task3 = BashOperator(\n",
    "        task_id='pwd',\n",
    "        bash_command='pwd',\n",
    "        dag=dag)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 4) task 의존 설정\n",
    "    - task1 후에 task2를 실행하고 싶다면\n",
    "        - task1.set_downstream(task2)\n",
    "        - task2.set_upstream(task1)\n",
    "    - 더 편해지면서 `>>`나 `<<` 사용 가능\n",
    "    - task1 >> task2로 사용 가능\n",
    "    - task1 >> [task2, task3]는 task1 후에 task2, task3 병렬 실행을 의미\n",
    "    \n",
    "    ```\n",
    "    task1 >> task2\n",
    "    task1 >> task3\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 5) DAG 파일을 DAG 폴더에 저장해 실행되는지 확인\n",
    "    - DAG 폴더에 넣고 바로 Webserver에 반영되진 않고 약간의 시간이 필요함\n",
    "    - 수정하고 싶으면 `~/airflow/airflow.cfg`에서 dagbag_import_timeout, dag_file_processor_timeout 값을 수정하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 6) 디버깅\n",
    "    - DAG이 실행되는지 확인 => 실행이 안된다면 DAG의 start_date를 확인\n",
    "    - 실행되서 초록색 불이 들어오길 기도\n",
    "    - 만약 초록이 아닌 빨간불이면 Task를 클릭해서 View log 클릭\n",
    "    - <img src=\"https://www.dropbox.com/s/s34pnvob61qmb5r/Screenshot%202020-02-12%2023.11.27.png?raw=1\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow BashOperator 사용하기\n",
    "    - 01-bash_operator.py 참고\n",
    "    - 앞에서 예제로 보여준 BashOperator 내용을 타이핑해보기 (5분)\n",
    "        - default_argument에서 start_date는 datetime(2019, 2, 13)\n",
    "        - DAG의 schedule_interval은 `0 10 * * *` 입력\n",
    "    - 파일명은 airflow_test.py\n",
    "    - (따로 설정 안했다면) `~/airflow/dags`에 저장하면 됨\n",
    "        - dags 폴더가 없다면 생성\n",
    "    - dags에 airflow_test.py 저장\n",
    "    - 지금은 간단한 bash command를 사용했지만, bash로 파이썬 파일도 실행할 수 있으니 활용 포인트가 무궁무진함\n",
    "    - 재실행하고 싶으면 Task 클릭 후 Clear 클릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- PythonOperator\n",
    "    - 02-python_operator.py 참고\n",
    "    - current_date를 받아서 한글로 요일 출력하는 함수 작성하기 (3분)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```\n",
    "from datetime import datetime\n",
    "\n",
    "def print_current_date():\n",
    "    date_kor = [\"월\",\"화\",\"수\",\"목\",\"금\",\"토\",\"일\"]\n",
    "    date_now = datetime.now().date()\n",
    "    datetime_weeknum = date_now.weekday()\n",
    "    print(f\"{date_now}는 {date_kor[datetime_weeknum]}요일입니다\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- PythonOperator(task_id, python_callable, op_args, dag, provide_context, templates_dict)로 사용함\n",
    "    - task_id는 task의 id(예 : print_current_date)\n",
    "    - python_callable는 호출 수 있는 python 함수를 인자로 넣음\n",
    "    - op_args : callable 함수가 호출될 때 사용할 함수의 인자\n",
    "    - dag : DAG 정의한 객체 넣으면 됨\n",
    "    - provide_context : True로 지정하면 Airflow에서 기본적으로 사용되는 keyword arguments 등이 사용 가능하게 됨\n",
    "    - templates_dict : op_args 등과 비슷하지만 jinja template이 변환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 방금 작성한 PythonOperator의 아쉬운 점\n",
    "    - 돌아간 Task의 로그를 보면 => 모두 같은 결과가 나옴\n",
    "        - 언제 실행해도 무조건 datetime.now()를 사용해서 현재 날짜를 사용함\n",
    "    - 어제 일자에서 이 함수를 실행했다면?\n",
    "        - 2020-02-13는 목요일입니다가 출력되었을 것\n",
    "    - 이런 경우 Python Code에서 시간에 대한 컨트롤을 가진 케이스\n",
    "    - Python Code에서 컨트롤을 가지면 과거 작업을 돌리기 힘듬\n",
    "    - Airflow에서 Date를 컨트롤하는게 좋음\n",
    "    - 이럴 때 Airflow에서 제공되는 기본 context 변수 또는 Jinja Template 사용\n",
    "        - Flask에서 Jinja Template을 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow의 기본 context 변수 사용하기\n",
    "    - 03-python_operator_with_context.py 참고\n",
    "    - PythonOperator에서 provide_context=True일 경우 사용 가능\n",
    "    - kwargs에 값이 저장됨\n",
    "    - 예를 들면\n",
    "    \n",
    "        ```\n",
    "        provide_context=True로 지정하면 kwargs 다양한 값들이 저장됨\n",
    "        {'dag': <DAG: python_dag_with_jinja>,\n",
    "        'ds': '2020-02-10',\n",
    "        'next_ds': '2020-02-11',\n",
    "        'next_ds_nodash': '20200211',\n",
    "        'prev_ds': '2020-02-09',\n",
    "        'prev_ds_nodash': '20200209',\n",
    "        'ds_nodash': '20200210',\n",
    "        'ts': '2020-02-10T00:30:00+00:00',\n",
    "        'ts_nodash': '20200210T003000',\n",
    "        'ts_nodash_with_tz': '20200210T003000+0000',\n",
    "        'yesterday_ds': '2020-02-09',\n",
    "        'yesterday_ds_nodash': '20200209',\n",
    "        'tomorrow_ds': '2020-02-11',\n",
    "        'tomorrow_ds_nodash': '20200211',\n",
    "        'end_date': '2020-02-10',\n",
    "        'execution_date': <Pendulum [2020-02-10T00:30:00+00:00]> ...}\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Jinja Template 사용하기\n",
    "    - 04-python_operator_with_jinja.py 참고\n",
    "    - `\"{{ ds }}\"` 이런 형태로 사용함 : execution_date\n",
    "    - PythonOperator는 기본 context 변수 사용이 더 쉽지만, 다른 Operator는 Jinja Template이 편함\n",
    "    - PythonOperator는 templates_dict에 변수를 넣어서 사용\n",
    "    - [Macros Default Variables](https://airflow.apache.org/docs/stable/macros.html#default-variables) Document에 정의되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Backfill\n",
    "    - Context Variable이나 Jinja Template을 사용하면 Backfill을 제대로 사용할 수 있음\n",
    "    - Backfill : 과거 날짜 기준으로 실행\n",
    "    - airflow backfill -s START_DATE -e END_DATE dag_id\n",
    "    - 아래 명령어를 입력해보고 Webserver에 가봅시다\n",
    "    \n",
    "```\n",
    "airflow backfill -s 2020-01-05 -e 2020-01-10 python_dag_with_jinja\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow로 토이 ETL 파이프라인 만들기\n",
    "    - 시나리오\n",
    "        - Google Cloud Storage에 매일 하루에 1번씩 주기적으로 csv 파일이 저장됨\n",
    "        - csv 파일을 BigQuery에 Load\n",
    "        - BigQuery에서 쿼리를 돌린 후, 일자별로 사용량 쿼리해서 Table 저장        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 설정 확인\n",
    "    - APIs & Services - Create Credentials - Service account\n",
    "    - Service account permissions에서 BigQuery Admin, Storage Admin \n",
    "    - Create key (optional) 밑에 있는 CREATE KEY 클릭\n",
    "        - JSON 선택하고 CREATE\n",
    "    - 다운로드된 project_name-123123.json 확인\n",
    "        - 이 Key는 매우 중요하니 꼭 잘 보관!!!(유출시 피해가 큼. 잘 모르면 그냥 삭제 추천)\n",
    "    - <img src=\"https://www.dropbox.com/s/1a3kzgtv3yytm42/Screenshot%202020-02-12%2020.58.38.png?raw=1\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow Webserver - Admin - connection 이동\n",
    "    - Conn Id가 google_cloud_default 찾기\n",
    "    - 왼쪽에 있는 연필 버튼 클릭\n",
    "    - Project Id에 자신의 프로젝트 ID 입력(name 아님!)\n",
    "        - [Google Cloud Platform Console](https://console.cloud.google.com/)에 있음\n",
    "    - Keyfile JSON에 아까 위에서 만든 JSON key 내용 통째로 복사해서 붙여넣기\n",
    "    - Scopes는 https://www.googleapis.com/auth/cloud-platform 입력\n",
    "    - Save 클릭\n",
    "    - <img src=\"https://www.dropbox.com/s/c6iqkgq19zqfjz7/Screenshot%202020-02-13%2022.01.40.png?raw=1\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Google Cloud Storage에 데이터 업로드\n",
    "    - [Google Cloud Storage](https://console.cloud.google.com/storage)로 이동\n",
    "    - CREATE BUCKET(이미 있다면 그거 사용해도 무방) 클릭\n",
    "        - 지역은 그냥 Region, us-east1하고 나머지 그냥 다 Continue 클릭\n",
    "        - 전 kyle-school bucket 만듬\n",
    "    - https://github.com/zzsza/kyle-school/tree/master/week6/data 데이터 다운!\n",
    "        - bike_data_20200209 ~ bike_data_20200212.csv\n",
    "    - 방금 만든 Bucket의 data 폴더 안에 방금 받은 파일 업로드\n",
    "- GoogleCloudStorageToBigQueryOperator, BigQueryOperator 사용할 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- GoogleCloudStorageToBigQueryOperator\n",
    "    - 05-simple_etl.py 참고\n",
    "    - file명은 일정한 특징이 있음. bike_data_{date}.csv\n",
    "    - schema_object : 스키마가 어떤 이름을 갖고, 어떤 타입인지 정의해둔 json \n",
    "    - bucket : 우리가 만든 bucket\n",
    "    - source_objects = Source Data\n",
    "    - destination_project_dataset_table : 저장할 경로\n",
    "    - 지금 bike_data_{date} Table 형태로 저장하는데, 이 형태는 샤딩으로 저장한 Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- BigQueryOperator\n",
    "    - 간단히 생각하면 쿼리를 날려서 Table에 저장\n",
    "    - agg_query에 간단한 쿼리 작성함\n",
    "    - BigQueryOperator는 destination_dataset_table만 잘 정의하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Webserver에서 이런 오류가 발생한다면\n",
    "    - No module named 'googleapiclient' \n",
    "   \n",
    "    ```\n",
    "    pip3 install --upgrade google-api-python-client\n",
    "    ```\n",
    "\n",
    "    - No module named 'airflow.gcp'\n",
    "    \n",
    "    ```\n",
    "    pip3 install 'apache-airflow[gcp]'==1.10.3\n",
    "    ```\n",
    "    \n",
    "    - 아마 웹서버쪽에서 werkzeug 다시 설치해야할 수 있음\n",
    "    \n",
    "    ```\n",
    "    pip3 install werkzeug==0.15.1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Airflow Local에서 실행할 경우\n",
    "    - SequentialExecutor : 동시에 1개만 처리 가능\n",
    "        - DB : sqlite\n",
    "    - 병렬로 돌리기 위해 postegre, redis 등을 붙임\n",
    "    - 설치가 매우 복잡하고 까다롭기 때문에 Docker 등을 활용하면 좋음\n",
    "    - 보통 회사엔 공용 Airflow가 띄워져 있고, 그걸 활용함\n",
    "    - 바닥부터 해봤으니, Airflow 단순히 사용하는 것은 꽤 익숙할 것이라 생각\n",
    "    - Jupyter Notebook으로 DAG 폴더를 연결하면, 친숙하게 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Already running on PID XXXX Error가 발생할 경우\n",
    "    - Webserver가 제대로 종료되지 않은 상황\n",
    "\n",
    "    ```\n",
    "    kill -9 $(lsof -t -i:8080)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 참고 자료\n",
    "- [Apache Airflow - Workflow 관리 도구(1)](https://zzsza.github.io/data/2018/01/04/airflow-1/)\n",
    "- [Awesome Apache Airflow Github](https://github.com/jghoman/awesome-apache-airflow)\n",
    "- [BigQuery non-partition Table을 partition Table로 옮기기](https://zzsza.github.io/gcp/2020/02/11/bigquery_query_to_partition_table/) : 아까 배운 샤딩 말고, 어떻게 하는지 확인해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 카일 스쿨\n",
    "- 각종 개발 도구들 & MLOps 개론\n",
    "    - 설문을 해주시면, 커리큘럼 개선에 도움이 됩니다\n",
    "    - 지금까지 받은 설문은 다 받고 고민하고 있습니다\n",
    "- 참고 : 3월 13일은 데이터 그룹 워크샵으로 카일 스쿨이 없습니다"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
